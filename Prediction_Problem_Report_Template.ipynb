{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Prediction Problem Interim Report Template\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c07d456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20413 entries, 0 to 20412\n",
      "Data columns (total 28 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   ID                                     20413 non-null  int64  \n",
      " 1   DIVISION_NUMBER                        20413 non-null  int64  \n",
      " 2   PRODUCT_NUMBER                         20413 non-null  int64  \n",
      " 3   PURCHASE_ORDER_DUE_DATE                20413 non-null  object \n",
      " 4   COMPANY_VENDOR_NUMBER                  20413 non-null  int64  \n",
      " 5   SHIP_FROM_VENDOR                       20413 non-null  int64  \n",
      " 6   ORDER_DATE                             20413 non-null  object \n",
      " 7   ORDER_DAY_OF_WEEK                      20413 non-null  int64  \n",
      " 8   PRODUCT_CLASSIFICATION                 20413 non-null  int64  \n",
      " 9   PURCHASE_ORDER_TYPE                    20413 non-null  int64  \n",
      " 10  DISTANCE_IN_MILES                      20413 non-null  float64\n",
      " 11  DIVISION_CODE                          20413 non-null  object \n",
      " 12  PURCHASE_FROM_VENDOR                   20413 non-null  int64  \n",
      " 13  AVERAGE_PRODUCT_ORDER_QUANTITY_MARKET  20413 non-null  float64\n",
      " 14  ORDER_QUANTITY_DEVIATION               20413 non-null  float64\n",
      " 15  TRANSIT_LEAD_TIME                      20413 non-null  float64\n",
      " 16  PURCHASING_LEAD_TIME                   20413 non-null  float64\n",
      " 17  DAYS_BETWEEN_ORDER_AND_DUE_DATE        20413 non-null  int64  \n",
      " 18  GIVEN_TIME_TO_LEAD_TIME_RATIO          20413 non-null  float64\n",
      " 19  DUE_DATE_WEEKDAY                       20413 non-null  int64  \n",
      " 20  PRODUCT_MARKET                         20413 non-null  int64  \n",
      " 21  RESERVABLE_INDICATOR                   20413 non-null  object \n",
      " 22  PRODUCT_STATUS                         20413 non-null  int64  \n",
      " 23  AVERAGE_DAILY_DEMAND_CASES             20372 non-null  float64\n",
      " 24  AVERAGE_VENDOR_ORDER_CYCLE_DAYS        20074 non-null  float64\n",
      " 25  AVERAGE_ORDER_CYCLE_DAYS               20074 non-null  float64\n",
      " 26  AVERAGE_ORDER_CYCLE_CASES              20074 non-null  float64\n",
      " 27  LEAD_TIME_TO_DISTANCE_RATIO            20413 non-null  float64\n",
      "dtypes: float64(11), int64(13), object(4)\n",
      "memory usage: 4.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20413 entries, 0 to 20412\n",
      "Data columns (total 2 columns):\n",
      " #   Column                Non-Null Count  Dtype\n",
      "---  ------                --------------  -----\n",
      " 0   ID                    20413 non-null  int64\n",
      " 1   ON_TIME_AND_COMPLETE  20413 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 319.1 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "X_train_path = \"train_X.csv\"\n",
    "y_train_path = \"train_y.csv\"\n",
    "\n",
    "X_train = pd.read_csv(X_train_path)\n",
    "y_train = pd.read_csv(y_train_path)\n",
    "\n",
    "# Display basic info about the datasets\n",
    "X_train.info(), y_train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b9670",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. This template serves as the required format for your code and report submission for the Prediction Problem assignment.\n",
    "2. You may modify the template to improve readability or add relevant details, but it must include all requested information.\n",
    "3. Ensure that your work is reproducible, meaning your code should consistently yield a metric value close to your Kaggle leaderboard score despite inherent randomness in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4b16",
   "metadata": {},
   "source": [
    "## 1) Exploratory Data Analysis (EDA)\n",
    "\n",
    "* Summarize key insights obtained from the dataset.\n",
    "* Discuss trends, correlations, or anomalies that influenced your modeling decisions.\n",
    "* Provide relevant data visualizations (e.g., histograms, scatter plots, correlation matrices) to support your findings."
   ]
  },
  {
   "cell_type": "raw",
   "id": "97873c5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## 2) Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4abb9",
   "metadata": {},
   "source": [
    "Describe any preprocessing steps performed on your dataset. This may include imputing missing values, creating dummy variables, combining levels of categorical variable(s), discarding predictors that are not useful, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efccf837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your data preparation code with comments here\n",
    "# The code should end when you obtain the data used for the model in Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3cec23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AVERAGE_DAILY_DEMAND_CASES          41\n",
       " AVERAGE_VENDOR_ORDER_CYCLE_DAYS    339\n",
       " AVERAGE_ORDER_CYCLE_DAYS           339\n",
       " AVERAGE_ORDER_CYCLE_CASES          339\n",
       " dtype: int64,\n",
       " ID                                 20413\n",
       " DIVISION_NUMBER                        3\n",
       " PRODUCT_NUMBER                      4498\n",
       " PURCHASE_ORDER_DUE_DATE              137\n",
       " COMPANY_VENDOR_NUMBER                344\n",
       " SHIP_FROM_VENDOR                     371\n",
       " ORDER_DATE                            89\n",
       " ORDER_DAY_OF_WEEK                      6\n",
       " PRODUCT_CLASSIFICATION                35\n",
       " PURCHASE_ORDER_TYPE                    2\n",
       " DIVISION_CODE                          3\n",
       " PURCHASE_FROM_VENDOR                 465\n",
       " DAYS_BETWEEN_ORDER_AND_DUE_DATE       65\n",
       " DUE_DATE_WEEKDAY                       7\n",
       " PRODUCT_MARKET                      5160\n",
       " RESERVABLE_INDICATOR                   1\n",
       " PRODUCT_STATUS                         1\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "missing_values = X_train.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Summarize categorical features\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'int64']).nunique()\n",
    "\n",
    "# Display missing values and categorical feature summary\n",
    "missing_values, categorical_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9cfe4",
   "metadata": {},
   "source": [
    "- Handle missing values.\n",
    "- Convert date columns into numerical features.\n",
    "- Encode categorical variables.\n",
    "- Scale numerical features and create interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f32d274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20413, 35)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from datetime import datetime\n",
    "\n",
    "# Drop non-informative columns\n",
    "X_train = X_train.drop(columns=['RESERVABLE_INDICATOR', 'PRODUCT_STATUS'])\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "X_train['PURCHASE_ORDER_DUE_DATE'] = pd.to_datetime(X_train['PURCHASE_ORDER_DUE_DATE'])\n",
    "X_train['ORDER_DATE'] = pd.to_datetime(X_train['ORDER_DATE'])\n",
    "\n",
    "# Create new date-related features\n",
    "X_train['ORDER_YEAR'] = X_train['ORDER_DATE'].dt.year\n",
    "X_train['ORDER_MONTH'] = X_train['ORDER_DATE'].dt.month\n",
    "X_train['ORDER_DAY'] = X_train['ORDER_DATE'].dt.day\n",
    "\n",
    "X_train['DUE_YEAR'] = X_train['PURCHASE_ORDER_DUE_DATE'].dt.year\n",
    "X_train['DUE_MONTH'] = X_train['PURCHASE_ORDER_DUE_DATE'].dt.month\n",
    "X_train['DUE_DAY'] = X_train['PURCHASE_ORDER_DUE_DATE'].dt.day\n",
    "\n",
    "# Create a feature for the number of days between order and due date\n",
    "X_train['ORDER_TO_DUE_DAYS'] = (X_train['PURCHASE_ORDER_DUE_DATE'] - X_train['ORDER_DATE']).dt.days\n",
    "\n",
    "# Drop original date columns\n",
    "X_train = X_train.drop(columns=['PURCHASE_ORDER_DUE_DATE', 'ORDER_DATE'])\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['DIVISION_NUMBER', 'PURCHASE_ORDER_TYPE', 'DIVISION_CODE']\n",
    "numerical_cols = [col for col in X_train.columns if col not in categorical_cols + ['ID']]\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Convert target variable to binary format\n",
    "y_train = y_train['ON_TIME_AND_COMPLETE'].astype(int)\n",
    "\n",
    "# Display transformed data shape\n",
    "X_processed.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a03db",
   "metadata": {},
   "source": [
    "## 3) Feature Engineering\n",
    "\n",
    "* List and explain the new features you created (if any).\n",
    "* Justify why these features were added and how they contribute to improving the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d864c3",
   "metadata": {},
   "source": [
    "Mention the logical/intuitive steps you took to obtain the final model. This may include identifying transformations, significant interactions, variable selection, etc. You do not need to put any code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## 4) Model Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a37864",
   "metadata": {},
   "source": [
    "Put your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6462944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the code that develops the model using the data you processed in Question 2, \n",
    "# and then uses the developed model on test data for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21f592",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6206898a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6882194464854274,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.66      0.80      0.72      2077\\n           1       0.74      0.57      0.64      2006\\n\\n    accuracy                           0.69      4083\\n   macro avg       0.70      0.69      0.68      4083\\nweighted avg       0.70      0.69      0.68      4083\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_processed, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_val)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "classification_rep = classification_report(y_val, y_pred)\n",
    "\n",
    "accuracy, classification_rep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416850b8",
   "metadata": {},
   "source": [
    "# Model 2 Adding Interaction Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adba62bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sherr\\OneDrive\\Desktop\\test_pip_env\\stat303_pip_env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7844722018123929,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.76      0.83      0.80      2077\\n           1       0.81      0.73      0.77      2006\\n\\n    accuracy                           0.78      4083\\n   macro avg       0.79      0.78      0.78      4083\\nweighted avg       0.79      0.78      0.78      4083\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Generate interaction terms\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_interactions = poly.fit_transform(X_processed)\n",
    "\n",
    "# Split data again after generating interaction terms\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_interactions, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression with interactions\n",
    "log_reg_inter = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_inter.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_inter = log_reg_inter.predict(X_val)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy_inter = accuracy_score(y_val, y_pred_inter)\n",
    "classification_rep_inter = classification_report(y_val, y_pred_inter)\n",
    "\n",
    "accuracy_inter, classification_rep_inter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c64202",
   "metadata": {},
   "source": [
    "Address Convergence Warning:\n",
    "\n",
    "Increase max_iter or switch to the saga solver.\n",
    "Apply feature scaling more aggressively to improve convergence.\n",
    "Regularization Tuning:\n",
    "\n",
    "Optimize C (regularization strength) in Logistic Regression.\n",
    "Compare L1 (Lasso) and L2 (Ridge) regularization to improve generalization.\n",
    "Feature Selection:\n",
    "\n",
    "Reduce dimensionality by selecting the most impactful features.\n",
    "Remove redundant interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a5061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sherr\\OneDrive\\Desktop\\test_pip_env\\stat303_pip_env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=10, max_iter=2000),\n",
       " 0.7822679402400196,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.76      0.83      0.80      2077\\n           1       0.81      0.73      0.77      2006\\n\\n    accuracy                           0.78      4083\\n   macro avg       0.78      0.78      0.78      4083\\nweighted avg       0.78      0.78      0.78      4083\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for Logistic Regression tuning\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'solver': ['lbfgs', 'saga'],  # Test different solvers\n",
    "    'max_iter': [2000]  # Increase iteration limit to address convergence issues\n",
    "}\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "log_reg_tuned = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "log_reg_tuned.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Get best model and evaluate on validation set\n",
    "best_model = log_reg_tuned.best_estimator_\n",
    "y_pred_best = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate best model\n",
    "accuracy_best = accuracy_score(y_val, y_pred_best)\n",
    "classification_rep_best = classification_report(y_val, y_pred_best)\n",
    "\n",
    "best_model, accuracy_best, classification_rep_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d26eb8",
   "metadata": {},
   "source": [
    "## 5) Model Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f47cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction and report the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf866d",
   "metadata": {},
   "source": [
    "Please note that your code for Questions 2, 4, and 5 will be executed sequentially, and it should generate a metric value close to the one displayed next to your name on the Kaggle leaderboard. While minor variations due to randomness in data science are expected, your implementation must be consistent and reproducible to receive full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24fde2d",
   "metadata": {},
   "source": [
    "## 6) Complete Code Submission\n",
    "\n",
    "* Provide a link to your Kaggle notebook or a GitHub repository containing your code.\n",
    "* Ensure the notebook is properly commented and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ceb3da",
   "metadata": {},
   "source": [
    "## 7) Reflection and Challenges\n",
    "\n",
    "* Discuss any challenges faced during this process and how they were addressed.\n",
    "* Mention any improvements you would make for the next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959eacf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat303_pip_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
